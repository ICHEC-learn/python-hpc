{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"../../fig/ICHEC_Logo.jpg\" alt=\"Drawing\" style=\"width: 500px;\"/>\n",
    "\n",
    "<center><img src=\"../../fig/notebooks/MPI_Logo.png\" alt=\"Drawing\" style=\"width: 250px;\"/>\n",
    "\n",
    "# <center>Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the template submission script below to submit your files to the specialised queue. Substitute the `{# MY_JOB #}`, `{# MY_FILE #}`, `{# MY_ENV #}`, `{# MY_COMMAND #}`, strings for appropriate job, file name, as well as correct commands and environment. \n",
    "\n",
    "```slurm\n",
    "#!/bin/bash\n",
    "#SBATCH --nodes=1\n",
    "#SBATCH --time=00:10:00\n",
    "#SBATCH -A course\n",
    "#SBATCH --job-name={# MY_JOB #}\n",
    "#SBATCH -p CourseDevQ\n",
    "#SBATCH --reservation=CourseMay\n",
    "\n",
    "\n",
    "module purge\n",
    "module load conda\n",
    "module list\n",
    "\n",
    "source activate {# MY_ENV #}\n",
    "\n",
    "cd $SLURM_SUBMIT_DIR\n",
    "\n",
    "{# MY_COMMAND #} {# MY_FILE #}.py\n",
    "\n",
    "exit 0\n",
    "```\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# <center> <b>MPI (Message Passing Interface)</b>\n",
    "    \n",
    "### <center><b>Exercise 1: Multiplying an array</b>\n",
    "\n",
    "#### <center> 5 minutes\n",
    "\n",
    "Using the code just covered in the lecture, found below, create a list from 1-4, and use a for loop to multiply the element of the list by the rank. Add a print statement to show how the array changes depending on the rank."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "```python\n",
    "from mpi4py import MPI\n",
    "\n",
    "# communicator containing all processes\n",
    "comm = MPI.COMM_WORLD \n",
    "\n",
    "size = comm.Get_size()\n",
    "rank = comm.Get_rank()\n",
    "\n",
    "# TODO: Create array and loop\n",
    "\n",
    "print(\"I am rank %d in group of %d processes\" % (rank, size))\n",
    "# TODO: Add print statement\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "### <center><b>Exercise 2: Pairwise Exchange</b>\n",
    "\n",
    "#### <center> 10 minutes\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copy the code block below into a file caled `pairwise_exchange.py`, implement a one-dimensional pairwise exchange of the ranks of n processes with periodic boundary conditions. That is, each process communicates its rank to each of its topological neighbours."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "from mpi4py import MPI\n",
    "\n",
    "comm = MPI.COMM_WORLD \n",
    "rank = comm.Get_rank()\n",
    "size = comm.Get_size()\n",
    "\n",
    "# TODO: Define neighbours\n",
    "\n",
    "# TODO: Use send and recv to \n",
    "\n",
    "print ('I am process ', rank, ', my neighbours are processes', left, ' and ', right)\n",
    "```\n",
    "\n",
    "For 4 processes, the output should be similar to;\n",
    "```\n",
    "I am process  1 , my neighbours are processes 0  and  2\n",
    "I am process  3 , my neighbours are processes 2  and  0\n",
    "I am process  0 , my neighbours are processes 3  and  1\n",
    "I am process  2 , my neighbours are processes 1  and  3\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### <center><b>Exercise 3: Parallel Sum</b>\n",
    "\n",
    "#### <center> 25 minutes\n",
    "    \n",
    "\n",
    "\n",
    "Implement the parallel sum case study. That is;\n",
    "- take the code block below\n",
    "- implement the appropriate send/receive to distribute the data from rank 0\n",
    "- perform the partial sum on each process\n",
    "- implement the appropriate send/receive to gather the data at rank 0\n",
    "- compute the final sum from the gathered data on rank 0\n",
    "\n",
    "(Bonus) For added difficulty (and best practice), try to implement the same approach with an arbitrary number of processes. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "from mpi4py import MPI\n",
    "import numpy as np\n",
    "\n",
    "comm = MPI.COMM_WORLD\n",
    "rank = comm.Get_rank()\n",
    "\n",
    "local_data = np.empty( , dtype=int)\n",
    "\n",
    "# Distribute data\n",
    "if rank==0:\n",
    "    data = np.arange(100, dtype=int)\n",
    "    local_data = data[] # TODO: choose appropriate range\n",
    "    # TODO: Communicate\n",
    "else:    \n",
    "    # TODO: Communicate\n",
    "\n",
    "# TODO: Compute local and partial sums\n",
    "local_sum = np.sum(local_data)\n",
    "\n",
    "# Collect data\n",
    "if rank==0:\n",
    "    partial_sums = np.empty( , dtype=int)\n",
    "    # TODO: Communicate\n",
    "    \n",
    "    # TODO: Calculate total sum\n",
    "    \n",
    "    # TODO: Print result\n",
    "else:\n",
    "    # TODO: Communicate\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### <center><b>Exercise 4: SendRecv pairwise exchange</b>\n",
    "\n",
    "\n",
    "#### <center> 10 minutes\n",
    "    \n",
    "\n",
    "Modify the pairwise exchange code from Exercise 2 to use the combined send/recv communication.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### <center><b>Exercise 5: Non-Blocking Communication in a Ring</b>\n",
    "\n",
    "\n",
    "#### <center> 10 minutes\n",
    "    \n",
    "    \n",
    "- A set of processes are arranged in a ring\n",
    "- 1. Each process stores its rank in MPI.COMM_WORLD into an integer variable, `snd_buf`\n",
    "- 2. Each process passes this `snd_buf` to the neighbour on its right\n",
    "- 3. Prepares for the next iteration\n",
    "- 4. Each processor calculates the sum of all values\n",
    "- Steps 2-4 are repeated with n number of times (number of processes)\n",
    "- Each process calculates the sum of all ranks\n",
    "\n",
    "The code below can work as we are only sending a small message. But it is still wrong.\n",
    "    \n",
    "Using a *synchronous send*, `Ssend`, will definitely cause a deadlock. Using a regular `Send` will at some point cause a deadlock, but not 100% of the time.\n",
    "     \n",
    "**NB: For this exercise only use `Issend`, which is used to demonstrate a deadlock is the non-blocking routine is incorrectly used. Real applications use `Isend`, not `Issend`**\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "```python\n",
    "from mpi4py import MPI\n",
    "import numpy as np\n",
    "\n",
    "rcv_buf = np.empty((), dtype=np.intc)\n",
    "#rcv_buf = np.empty((), dtype=np.intc) # uninitialized 0 dimensional integer array\n",
    "status = MPI.Status()\n",
    "\n",
    "comm = MPI.COMM_WORLD\n",
    "my_rank = comm.Get_rank()\n",
    "size = comm.Get_size()\n",
    "\n",
    "right = (my_rank+1) % size\n",
    "left  = (my_rank-1+size) % size\n",
    "\n",
    "sum = 0\n",
    "snd_buf = np.array(my_rank, dtype=np.intc) # 0 dimensional integer array with 1 element initialized with the value of my_rank\n",
    "\n",
    "for i in range(size):\n",
    "   comm.Send((snd_buf, 1, MPI.INT), dest=right, tag=17)\n",
    "   \n",
    "   #request = comm.Isend((snd_buf, 1, MPI.INT), dest=right, tag=17)\n",
    "\n",
    "\n",
    "   comm.Recv((rcv_buf, 1, MPI.INT), source=left,  tag=17, status=status)\n",
    "   #request.Wait(status)\n",
    "   np.copyto(snd_buf, rcv_buf) # We make a copy here. What happens if we write snd_buf = rcv_buf instead?\n",
    "   sum += rcv_buf\n",
    "print(f\"PE{my_rank}:\\tSum = {sum}\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### <center><b>Exercise 6a: P2P to Gather\n",
    "\n",
    "\n",
    "#### <center> 15 minutes\n",
    "    \n",
    "\n",
    "The Send and Receive code here works perfectly as is, but using `gather` would certainly make things much cleaner, and we only have to call one MPI command.\n",
    "    \n",
    "Substitute the point-to-point communication with one call to `gather`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "```python\n",
    "from mpi4py import MPI\n",
    "import numpy as np\n",
    "\n",
    "comm = MPI.COMM_WORLD\n",
    "my_rank = comm.Get_rank()\n",
    "num_procs = comm.Get_size()\n",
    "\n",
    "# Each process assigned some work\n",
    "res = np.array(my_rank**2, dtype=np.double)\n",
    "print(f\"I am process {my_rank} out of {num_procs}, result={res:f}\")\n",
    "if (my_rank == 0):\n",
    "   res_arr = np.empty(num_procs, dtype=np.double)\n",
    "\n",
    "# TODO: Substitute the following block with MPI_Gather\n",
    "###\n",
    "if (my_rank != 0):  \n",
    "   # Sending some results from all processes (except 0) to process 0:\n",
    "   comm.Send((res, 1, MPI.DOUBLE), dest=0, tag=99)\n",
    "else: \n",
    "   res_arr[0] = res # process 0's own result\n",
    "   # Receive all the messages\n",
    "   for rank in range(1,num_procs):\n",
    "      # Result of processes 1 -> n\n",
    "      comm.Recv((res_arr[rank:], 1, MPI.DOUBLE), source=rank, tag=99, status=None) \n",
    "###\n",
    "\n",
    "if (my_rank == 0):\n",
    "   for rank in range(num_procs):\n",
    "      print(f\"I'm proc 0: result of process {rank} is {res_arr[rank]:f}\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <center><b>Exercise 6b (Optional): Parallel Sum with Collective Communication \n",
    "\n",
    "Modify the parallel sum code from the second exercise to use replace the point-to-point communication with collective communication. Preferably, this should abstracted to an arbitrary number of processes. Try with different methods, which one(s) produce the expected results?\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### <center><b>Exercise 7: Global reduction \n",
    "    \n",
    "#### <center> 10 minutes </center>\n",
    "    \n",
    "The communication around a ring covered in Exercise 5 can be further simplified using a global reduction. modify your solution to Exercise 5 by:\n",
    "    \n",
    "- Determine the quantity of code that needs to be replaced by the global reduction\n",
    "- Using `Allreduce` to call the collective reduction routine\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### <center><b>Exercise 8: Communicators\n",
    "    \n",
    "#### <center> 20 minutes </center>\n",
    "    \n",
    "Modify the Allreduce program. You will need to split the communicator MPI_COMM_WORLD into 1/3 and 2/3, so the color variable for the input for `Split` is;\n",
    "    \n",
    "color = (rank > [$\\frac{size -1}{3}$])\n",
    "    \n",
    "- Calculate **sumA** and **sumB** over all processes within each sub-communicator\n",
    "- Run with 12 processes to produce the following results:\n",
    "    - **sumA:** Sums up ranks in MPI_COMM_WORLD: 4 & 8 split with world ranks;\n",
    "        - 0 -> 3  = 6\n",
    "        - 4 -> 11 = 60 \n",
    "    - **sumB:** Sums up ranks within new sub-communicators: 4 & 8 split with sub-comm ranks:\n",
    "        - 0 -> 3  = 6\n",
    "        - 0 -> 7  = 28"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "```python\n",
    "sumA = np.empty((), dtype=np.intc)\n",
    "sumB = np.empty((), dtype=np.intc)\n",
    "\n",
    "comm = MPI.COMM_WORLD\n",
    "size = comm.Get_size()\n",
    "rank = np.array(comm.Get_rank(), dtype=np.intc)\n",
    "\n",
    "# TODO\n",
    "# 1. Define 'color' variable\n",
    "# 2. Define your sub-communicator using a split\n",
    "# 3. Define your new size and ranks\n",
    "# 4. Keep track of your variable names for clarity\n",
    "\n",
    "# Compute sum of all ranks.\n",
    "comm.Allreduce(rank, (sumA, 1, MPI.INT), MPI.SUM) \n",
    "comm.Allreduce(rank, (sumB, 1, MPI.INT), MPI.SUM)\n",
    "\n",
    "print(\"PE world:{:3d}, color={:d} sub:{:3d}, SumA={:3d}, SumB={:3d} in comm_world\".format( \n",
    "          rank, \"TODO: color\", \"TODO: sub_rank\", sumA, sumB))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### <center><b> Easier Exercise: Collective Communication </b></center>\n",
    "\n",
    "- In this exercise you will use different routines for collective communication. Use the skeleton code to get started.\n",
    "  \n",
    "  \n",
    "A) First, write a program where rank 0 sends an array containing integers from 0 to 7 to all other ranks using collective communication. Use four cores.\n",
    "[Hint: Use broadcasting]\n",
    "\n",
    "- From these arrays create the initial arrays shown below.\n",
    "\n",
    "B)\n",
    "\n",
    "|        |  |  |  |  |  |  |  |  |\n",
    "|--------|--|--|--|--|--|--|--|--|\n",
    "|Task 0: | 0| 1| 2| 3| 4| 5| 6| 7|\n",
    "|Task 1: | 8| 9|10|11|12|13|14|15|\n",
    "|Task 2: |16|17|18|19|20|21|22|23|\n",
    "|Task 3: |24|25|26|27|28|29|30|31|\n",
    "\n",
    "\n",
    "- Each task should receive a buffer for eight elements with each one initialised to -1. \n",
    "\n",
    "- Implement a program that sends and receives values from the data arrays to receive buffers using single collective routines so that the receive buffers will have the following values;\n",
    "\n",
    "C)\n",
    "\n",
    "|        |  |  |  |  |  |  |  |  |\n",
    "|--------|--|--|--|--|--|--|--|--|\n",
    "|Task 0: | 0| 1|-1|-1|-1|-1|-1|-1|\n",
    "|Task 1: | 2| 3|-1|-1|-1|-1|-1|-1|\n",
    "|Task 2: | 4| 5|-1|-1|-1|-1|-1|-1|\n",
    "|Task 3: | 6| 7|-1|-1|-1|-1|-1|-1|\n",
    "\n",
    "- [Hint: Use `comm.Scatter()`]\n",
    "\n",
    "D)\n",
    "\n",
    "|        |  |  |  |  |  |  |  |  |\n",
    "|--------|--|--|--|--|--|--|--|--|\n",
    "|Task 0: |-1|-1|-1|-1|-1|-1|-1|-1|\n",
    "|Task 1: | 0| 1| 8| 9|16|17|24|25|\n",
    "|Task 2: |-1|-1|-1|-1|-1|-1|-1|-1|\n",
    "|Task 3: |-1|-1|-1|-1|-1|-1|-1|-1|\n",
    "\n",
    "- [Hint: Use `comm.Gather()`]\n",
    "\n",
    "E)\n",
    "\n",
    "|        |  |  |  |  |  |  |  |  |\n",
    "|--------|--|--|--|--|--|--|--|--|\n",
    "|Task 0: | 8|10|12|14|16|18|20|22|\n",
    "|Task 1: |-1|-1|-1|-1|-1|-1|-1|-1|\n",
    "|Task 2: |40|42|44|46|48|50|52|54|\n",
    "|Task 3: |-1|-1|-1|-1|-1|-1|-1|-1|\n",
    "\n",
    "- [Hint: Create two communicators and use `comm.Reduce()`]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "```python\n",
    "from mpi4py import MPI\n",
    "import numpy\n",
    "from sys import stdout\n",
    "\n",
    "comm = MPI.COMM_WORLD\n",
    "rank = comm.Get_rank()\n",
    "size = comm.Get_size()\n",
    "\n",
    "assert size == 4, 'Number of MPI tasks has to be 4.'\n",
    "\n",
    "if rank == 0:\n",
    "    print('A)Broadcast:')\n",
    "\n",
    "# TODO: create data vector at task 0 and send it to everyone else\n",
    "#       using collective communication\n",
    "if rank == 0:\n",
    "    data = ...\n",
    "else:\n",
    "    data = ...\n",
    "...\n",
    "print('  Task {0}: {1}'.format(rank, data))\n",
    "\n",
    "\n",
    "# Prepare data vectors ..\n",
    "data = ...  # TODO: create the data vectors\n",
    "# .. and receive buffers\n",
    "buff = numpy.full(8, -1, int)\n",
    "\n",
    "# ... wait for every rank to finish ...\n",
    "comm.barrier()\n",
    "if rank == 0:\n",
    "    print('')\n",
    "    print('-' * 32)\n",
    "    print('')\n",
    "    print('B) Initial Data vectors:')\n",
    "print('  Task {0}: {1}'.format(rank, data))\n",
    "comm.barrier()\n",
    "if rank == 0:\n",
    "    print('')\n",
    "    print('-' * 32)\n",
    "    print('')\n",
    "    print('C) Scatter:')\n",
    "\n",
    "# TODO: how to get the desired receive buffer using a single collective\n",
    "#       communication routine?\n",
    "...\n",
    "print('  Task {0}: {1}'.format(rank, buff))\n",
    "\n",
    "# ... wait for every rank to finish ...\n",
    "buff[:] = -1\n",
    "comm.barrier()\n",
    "if rank == 0:\n",
    "    print('')\n",
    "    print('-' * 32)\n",
    "    print('')\n",
    "    print('D) Gather:')\n",
    "\n",
    "# TODO: how to get the desired receive buffer using a single collective\n",
    "#       communication routine?\n",
    "\n",
    "...\n",
    "print('  Task {0}: {1}'.format(rank, buff))\n",
    "\n",
    "# ... wait for every rank to finish ...\n",
    "buff[:] = -1\n",
    "comm.barrier()\n",
    "if rank == 0:\n",
    "    print('')\n",
    "    print('e)')\n",
    "\n",
    "# TODO: how to get the desired receive buffer using a single collective\n",
    "#       communication routine?\n",
    "...\n",
    "print('  Task {0}: {1}'.format(rank, buff))\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "***\n",
    "\n",
    "## <center><b>Heat Equation<b/>\n",
    "    \n",
    "The series of exercises covered here applies the methods covered during the lectures to the heat equation we have seena few time before. These can be completed in your own time or if you finish an exercise early, give these a go and see how you fare. The codes to modify everything can be found in [heat_equation.ipynb](./heat_equation/heat_equation.ipynb). The solutions for these are available in [soln](./soln).\n",
    "\n",
    "***\n",
    "### <center><b>Advanced Exercise 1: Point to Point</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"../img/4.1.1.png\" alt=\"Drawing\" style=\"width: 350px;\"/> </center>\n",
    "\n",
    "- **<span style=\"color:green\">halo region</span>**\n",
    "- **<span style=\"color:purple\">border region</span>**\n",
    "- **<span style=\"color:blue\">local region</span>**\n",
    "\n",
    "Modify the [heat equation codebase](./heat_equation/heat_equation.ipynb) below to perform one-dimensional domain decomposition using MPI. This involves;\n",
    "- splitting the grid between each process\n",
    "- maintain halo regions as indicated in the above image for storing neighbouring border data\n",
    "- perform an exchange of border data before each iteration of the grid (i.e, border data is sent to the halo region of the appropriate neighbour)\n",
    "- transfer the final grid back to the root rank \n",
    "\n",
    "To do this, you will need to;\n",
    "1. implement the `halo_exchange` function in `modules/comms.py`.\n",
    "2. implement the `distribute_subfields` and `gather_subfields` functions in `modules/comms.py`. These should distribute the field from the rank 0 process to the rest and from the rest to the rank 0 process respectively.\n",
    "2. modify `main.py` `main` function to only initialise the field on rank 0. Call the `distribute_subfields` and `gather_subfields` functions where appropriate. Only print and write to files from the rank 0 process.\n",
    "4. modify the `iterate` function in `main.py` to call the `halo_exchange` function before each iteration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### <center><b>Advanced Exercise 2: Non-Blocking Communication</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Further modify the [heat equation codebase](./heat_equation/heat_equation.ipynb) to use non-blocking communication for the halo exchange. Make sure not to access transferred data without waiting for it to arrive!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### <center><b>Advanced Exercise 3: Collective Communication</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modify the grid distribution strategy used in the [heat equation codebase](./heat_equation/heat_equation.ipynb) to use collective communication. This will involve changes to the `distribute_subfields` and `gather_subfields` functions in `modules/comms.py`."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
