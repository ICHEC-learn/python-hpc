{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"../img/ICHEC_Logo.png\" alt=\"Drawing\" style=\"width: 500px;\"/></center>\n",
    "\n",
    "<center><img src=\"../img/MPI_Logo.png\" alt=\"Drawing\" style=\"width: 250px;\"/>\n",
    "\n",
    "# <center>MPI (Message Passing Interface)</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## <center><b>Non-blocking Communication</b></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's recall our deadlock situation, which can happen for a cyclic structure.\n",
    "\n",
    "<center><img src=\"../img/deadlock.png\" alt=\"Drawing\" style=\"width: 350px;\"/> </center>\n",
    "\n",
    "We also have a serialisation case, for non cyclic workflows\n",
    "\n",
    "<center><img src=\"../img/serialMPI.png\" alt=\"Drawing\" style=\"width: 350px;\"/> </center>\n",
    "\n",
    "<details>\n",
    "    <summary markdown=\"span\">Which situation is worse, deadlock or serialisation?</summary>\n",
    "<br>\n",
    "   Serialisation! \n",
    "   \n",
    "   At least with a deadlock its easier to identify the problem\n",
    "<br>\n",
    "</details>\n",
    "\n",
    "\n",
    "* Non-blocking sends and receives.\n",
    "    - **`isend`** & **`irecv`**\n",
    "    - Returns immediately and sends/receives in background.\n",
    "    - Return value is a Request object.\n",
    "* Enables some computing concurrently with communication.\n",
    "* Avoids many common deadlock situations.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "rank = comm.Get_rank()\n",
    "size = comm.Get_size()\n",
    "\n",
    "if rank == 0:\n",
    "    data = np.arange(size, dtype=float) * (rank + 1)\n",
    "    # start a send\n",
    "    req = comm.Isend(data, dest=1)\n",
    "    # ... do something else ...\n",
    "    calculate_something(rank)\n",
    "    # wait for the send to finish\n",
    "    req.wait()\n",
    "    # now safe to read/write data again\n",
    "\n",
    "elif rank == 1:\n",
    "    data = np.empty(size, dtype=float)\n",
    "    # post a recieve\n",
    "    req = comm.Irecv(data, source=0)\n",
    "    # ... do something else ...\n",
    "    calculate_something(rank)\n",
    "    # wait for the receive to finish\n",
    "    req.wait()\n",
    "    # data is not ready to use\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`isend` provides the message that is then communicated and received during `recv`. \n",
    "\n",
    "<center><img src=\"../img/isend.png\" alt=\"Drawing\" style=\"width: 350px;\"/> </center>\n",
    "\n",
    "`irecv` sets up the receive buffer, then `send` sends the message, but may have already received the message from another process. Or it can be received latest in the `wait`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<center><img src=\"../img/irecv.png\" alt=\"Drawing\" style=\"width: 350px;\"/> </center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"../img/4.1.1.png\" alt=\"Drawing\" style=\"width: 350px;\"/> </center>\n",
    "\n",
    "* In above example, where data colour coded as;\n",
    "    - **<span style=\"color:green\">ghost_data</span>**\n",
    "    - **<span style=\"color:purple\">border_data</span>**\n",
    "    - **<span style=\"color:blue\">local_data</span>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "request = comm.Irecv(ghost_data)\n",
    "request2 = comm.Isend(border_data)\n",
    "compute(ghost_independent_data)\n",
    "request.wait()\n",
    "compute(border_data)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Interleaving communication and computation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Methods **`waitall()`** and **`waitany()`** come in handy when dealing with multiple non-blocking operations (available in the **`MPI.Request`** class).\n",
    "* **`Request.waitall(requests)`**\n",
    "    - Wait for all initiated requests to complete.\n",
    "* **`Request.waitany(requests)`**\n",
    "    - Wait for any initiated request to complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile Isend_Irecv.py \n",
    "\n",
    "from mpi4py import MPI\n",
    "from mpi4py.MPI import Request\n",
    "import numpy as np\n",
    "\n",
    "comm = MPI.COMM_WORLD\n",
    "rank = comm.Get_rank()\n",
    "\n",
    "# data = send buffer\n",
    "data = np.arange(10, dtype=float) * (rank + 1)\n",
    "# buffer = receive buffer\n",
    "buffer = np.zeros(10, dtype=float)\n",
    "\n",
    "print('rank', rank, 'sending:', data)\n",
    "\n",
    "if rank == 0:\n",
    "    req = [comm.Isend(data, dest=1)]\n",
    "    req.append(comm.Irecv(buffer, source=1))\n",
    "if rank == 1:\n",
    "    req = [comm.Isend(data, dest=0)]\n",
    "    req.append(comm.Irecv(buffer, source=0))\n",
    "    \n",
    "print('rank', rank, 'receive buffer before wait:', buffer)\n",
    "    \n",
    "Request.waitall(req)\n",
    "\n",
    "print('rank', rank, 'receive buffer after wait:', buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mpirun -np 2 python3 Isend_Irecv.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Non-blocking communication is usually the smart way to do point-to-point communication in MPI.\n",
    "* Non-blocking communication realisation.\n",
    "    - **`isend`** / **`Isend`**\n",
    "    - **`irecv`** / **`Irecv`**\n",
    "    - **`request.wait()`**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <center> [Exercise 5](./05-Exercises-MPI.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## <center><b>Collective Communication</b></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Collective communication transmits data among all processes in a process group (communicator).\n",
    "    - These routines must be called by all the processes in the group.\n",
    "    - Amount of sent and received data must match.\n",
    "* Collective communication includes\n",
    "    - Data movement\n",
    "    - Collective computation\n",
    "    - Synchronization\n",
    "* Example\n",
    "    - **`comm.barrier()`** makes every task hold until all tasks in the communicator comm have called it.\n",
    "* Collective communication typically outperforms point-to-point communication."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Original code can reduce from;\n",
    "\n",
    "```python\n",
    "if rank == 0:\n",
    "    for i in range(1, size):\n",
    "        comm.Send(data, i)\n",
    "else:\n",
    "    comm.Recv(data, 0)\n",
    "```\n",
    "to only one line..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will cover 4 types of collective communication"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Broadcasting\n",
    "\n",
    "* Code becomes more compact (and efficient!) and easier to maintain:\n",
    "    - For example, communicating a NumPy array from task 0 to all other tasks:\n",
    "* Send the same data from one process to all the others\n",
    "\n",
    "```python\n",
    "comm.Bcast(data, 0)\n",
    "```\n",
    "<center><img src=\"../img/4.1.3.png\" alt=\"Drawing\" style=\"width: 450px;\"/> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile broadcast.py\n",
    "\n",
    "from mpi4py import MPI\n",
    "import numpy as np\n",
    "\n",
    "comm = MPI.COMM_WORLD\n",
    "rank = comm.Get_rank()\n",
    "\n",
    "if rank == 0:\n",
    "    # Python object\n",
    "    py_data = {'key1' : 0.0, 'key2' : 11}\n",
    "    # NumPy array\n",
    "    data = np.arange(8)/10\n",
    "else:\n",
    "    py_data = None\n",
    "    data = np.zeros(8)\n",
    "\n",
    "# Broadcasting the python object\n",
    "new_data = comm.bcast(py_data, root=0)\n",
    "print('rank', rank, 'received python object:', new_data)\n",
    "# Broadcasting the NumPy array\n",
    "comm.Bcast(data, root=0)\n",
    "print('rank', rank, 'received NumPy array:', data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mpirun -np 4 python3 broadcast.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scattering\n",
    "\n",
    "* Sends/distributes equal amounts of data from one process to others.\n",
    "* Segments $A$, $B$, $...$ may contain multiple elements\n",
    "\n",
    "<center><img src=\"../img/4.1.4.png\" alt=\"Drawing\" style=\"width: 450px;\"/> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "%%writefile scatter.py\n",
    "\n",
    "from mpi4py import MPI\n",
    "import numpy as np\n",
    "\n",
    "comm = MPI.COMM_WORLD\n",
    "rank = comm.Get_rank()\n",
    "size = comm.Get_size()\n",
    "\n",
    "if rank == 0:\n",
    "    # Python object\n",
    "    py_data = range(size)\n",
    "    data = np.arange(size**2, dtype=float)\n",
    "else:\n",
    "    py_data = None\n",
    "    data = None\n",
    "\n",
    "# Scatter the python object\n",
    "new_data = comm.scatter(py_data, root=0)\n",
    "print('rank', rank, 'received python object:', new_data)\n",
    "\n",
    "# Scatter the NumPy array\n",
    "# A receive buffer is needed here!\n",
    "buffer = np.empty(size, dtype=float)\n",
    "comm.Scatter(data, buffer, root=0)\n",
    "print('rank', rank, 'received NumPy array:', data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mpirun -np 4 python3 broadcast.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gathering\n",
    "\n",
    "* Collect/pulls data from all the processes into one process\n",
    "* Segments $A$,$B$, $...$ may contain multiple elements\n",
    "\n",
    "<center><img src=\"../img/4.1.5.png\" alt=\"Drawing\" style=\"width: 450px;\"/> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "%%writefile gather.py\n",
    "\n",
    "from mpi4py import MPI\n",
    "import numpy as np\n",
    "\n",
    "comm = MPI.COMM_WORLD\n",
    "rank = comm.Get_rank()\n",
    "size = comm.Get_size()\n",
    "\n",
    "data = np.arange(10, dtype=float) * (rank + 1)\n",
    "\n",
    "# Gather the value of rank from each rank, then send to rank 0\n",
    "n = comm.gather(rank, root=0)\n",
    "\n",
    "# Gather the NumPy array from each rank, then send to rank 0\n",
    "buffer = np.zeros(size * 10, dtype=float)\n",
    "comm.Gather(data, buffer, root=0)\n",
    "\n",
    "if rank == 0:\n",
    "    print('gathered ranks:', n)\n",
    "    print('gathered NumPy arrays:', buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mpirun -np 4 python3 gather.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <center>[Exercise 6](./05-Exercises-MPI.ipynb)</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### <center><b>Reduction Operation</b></center>\n",
    "\n",
    "* Applies an operation over a set of processes and places the result in a single process\n",
    "\n",
    "<center><img src=\"../img/4.1.6.png\" alt=\"Drawing\" style=\"width: 450px;\"/> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we will look at reduce routines for a single integer (rank) and a numpy array, which on each process is [0,1,2,3] * (rank + 1)\n",
    "\n",
    "What would be the expected result for running 3 processes for\n",
    "\n",
    "<details>\n",
    "    <summary markdown=\"span\">Rank? </summary>\n",
    "   3\n",
    "<br>\n",
    "</details>\n",
    "<br>\n",
    "<details>\n",
    "    <summary markdown=\"span\">And the numpy array? </summary>\n",
    "   [0,6,12,18]\n",
    "<br>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile reduce.py\n",
    "\n",
    "from mpi4py import MPI\n",
    "import numpy as np\n",
    "\n",
    "comm = MPI.COMM_WORLD\n",
    "rank = comm.Get_rank()\n",
    "size = comm.Get_size()\n",
    "\n",
    "data = np.arange(10, dtype=float) * (rank + 1)\n",
    "print(data)\n",
    "# Gather the value of rank from each rank, then send to rank 0\n",
    "n = comm.reduce(rank, root=0)\n",
    "\n",
    "# Gather ther NumPy array from each rank, then send to rank 0\n",
    "buffer = np.zeros(10, dtype=float)\n",
    "comm.Reduce(data, buffer, root=0)\n",
    "\n",
    "if rank == 0:\n",
    "    print('rank reduction:', n)\n",
    "    print('NumPy reduction:', buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mpirun -np 4 python3 reduce.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other Common Collective Operations\n",
    "\n",
    "* **`Scatterv`**: Each process receives different amount of data.\n",
    "* **`Gatherv`**: Each process sends different amount of data.\n",
    "* **`Allreduce`**: All processes receive the results of reduction.\n",
    "* **`Alltoall`**: Each process sends and receives to/from each other\n",
    "* **`Alltoallv`**: Each process sends and receives different amount of data to/from each other"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Non-blocking Collectives\n",
    "\n",
    "* A feature in MPI 3: <span style=\"color:red\">But no support in `mpi4py`</span>\n",
    "* Non-blocking collectives enable the overlapping of communication and computation together with the benefits of collective communication.\n",
    "* Restrictions:\n",
    "    - Have to be called in same order by all ranks in a communicator.\n",
    "    - Mixing of blocking and non-blocking collectives is not allowed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Common Mistakes with Collectives\n",
    "\n",
    "* Using a collective operation within one branch of an if-test of the rank.\n",
    "    - **`if rank == 0: comm.bcast(...)`**\n",
    "    - All processes in a communicator must call a collective routine!\n",
    "* Assuming that all processes making a collective call would complete at the same time.\n",
    "* Using the input buffer as the output buffer:\n",
    "    - **`comm.Scatter(a, a, MPI.SUM)`**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <center>[Exercise 7](./05-Exercises-MPI.ipynb)</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## <center><b>Communicators</b></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"../img/4.1.2.png\" alt=\"Drawing\" style=\"width: 350px;\"/> </center>\n",
    "\n",
    "* By default a single, universal communicator exists to which all processes belong (**`MPI.COMM_WORLD`**).\n",
    "* One can create new communicators, e.g. by splitting this into sub-groups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile comm_ranks.py\n",
    "\n",
    "from mpi4py import MPI\n",
    "import numpy as np\n",
    "\n",
    "from mpi4py import MPI\n",
    "comm = MPI.COMM_WORLD\n",
    "rank = comm.Get_rank()\n",
    "\n",
    "color = rank % 4\n",
    "\n",
    "local_comm = comm.Split(color)\n",
    "local_rank = local_comm.Get_rank()\n",
    "\n",
    "print(\"Global rank: %d Local rank: %d\" % (rank, local_rank))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mpirun -np 8 python3 comm_ranks.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <center>[Exercise 8](./05-Exercises-MPI.ipynb)</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "## <center><b>Other MPI Routines & Methods</b></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Non-blocking Collectives\n",
    "\n",
    "* A feature in MPI 3: <span style=\"color:red\">But no support in `mpi4py`</span>\n",
    "* Non-blocking collectives enable the overlapping of communication and computation together with the benefits of collective communication.\n",
    "* Restrictions:\n",
    "    - Have to be called in same order by all ranks in a communicator.\n",
    "    - Mixing of blocking and non-blocking collectives is not allowed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Common Mistakes with Collectives\n",
    "\n",
    "* Using a collective operation within one branch of an if-test of the rank.\n",
    "    - **`if rank == 0: comm.bcast(...)`**\n",
    "    - All processes in a communicator must call a collective routine!\n",
    "* Assuming that all processes making a collective call would complete at the same time.\n",
    "* Using the input buffer as the output buffer:\n",
    "    - **`comm.Scatter(a, a, MPI.SUM)`**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "* Collective communications involve all the processes within a communicator.\n",
    "    - All processes must call them.\n",
    "* Collective operations make code more transparent and compact.\n",
    "* Collective routines allow optimizations by MPI library.\n",
    "* MPI-3 also contains non-blocking collectives, but these are currently not supported by MPI for Python.\n",
    "* Documentation for `mpi4py` is quite limited\n",
    "    - MPI used in C, C++, Fortran, and ideally not suited for python as a whole\n",
    "    - If you are serious about MPI, we suggest utilising a different language of your choice\n",
    "        - Leads to better performance as a result\n",
    "        \n",
    "<center><img src=\"../img/4.1.7.png\" alt=\"Drawing\" style=\"width: 450px;\"/> </center>\n",
    "\n",
    "Performance of `mpi4py` using for a ping-pong test\n",
    "\n",
    "* It is possible but not recommended to communicate arbitrary Python objects\n",
    "* NumPy arrays can be communicated with nearly the same speed as in C/Fortran"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Useful Links\n",
    "\n",
    "* [Read the docs](https://mpi4py.readthedocs.io/en/stable/index.html)\n",
    "* [\"A Python Introduction to Parallel Programming with MPI\"](https://materials.jeremybejarano.com/MPIwithPython/): _Jeremy Bejarano_\n",
    "* [mpi4py examples](https://github.com/jbornschein/mpi4py-examples): _JÃ¶rg Bornschein_\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
