{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"../img/ICHEC_Logo.png\" alt=\"Drawing\" style=\"width: 500px;\"/>\n",
    "\n",
    "<center><img src=\"../img/MPI_Logo.png\" alt=\"Drawing\" style=\"width: 250px;\"/>\n",
    "\n",
    "# <center>MPI (Message Passing Interface)</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## <center> <b>Processes vs. Threads</b>\n",
    "\n",
    "<details>\n",
    "    <summary markdown=\"span\"><b>What is a thread?</b></summary>\n",
    "<br>\n",
    "   - A dispatchable unit of work <b>within</b> a process<br>\n",
    "   - Lightweight operation that use the memory of the process they belong to<br>\n",
    "   - Threads share the same memory with other threads of the same process<br>\n",
    "<br>\n",
    "</details>\n",
    "    \n",
    "<center><img src=\"../img/Threads.png\" alt=\"Drawing\" style=\"width: 400px;\"/> <center/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary markdown=\"span\"><b>What is a process?</b></summary>\n",
    "<br>\n",
    "   - An instance of a program running on a computer<br>\n",
    "   - Heavyweight operation as every process has its own memory space<br>\n",
    "   - Processes don't share the memory with other processes<br>\n",
    "<br>\n",
    "</details>\n",
    "\n",
    "\n",
    "<center><img src=\"../img/Process.png\" alt=\"Drawing\" style=\"width: 250px;\"/> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## <center> <b>Introduction to MPI</b>\n",
    "\n",
    "Using MPI is the true way to achieve parallelism.\n",
    "\n",
    "<details>\n",
    "    <summary markdown=\"span\"><b>What is MPI?</b></summary>\n",
    "<br>\n",
    "   - An application programming interface (API) for communication between separate processes<br>\n",
    "   - MPI standards defines C, C++, Fortran interfaces\n",
    "   - Imported in Python using the <b>unofficial</b> <code>mpi4py</code> module<br>\n",
    "   - Programs with MPI are portable and scalable to run on tens to tens of thousands of cores<br>\n",
    "   - Over 300 procedures are possible, but only ~10 are really needed<br>\n",
    "   - The program is launched as separate processes <b>tasks</b> each with their own address space<br>\n",
    "   - Created in 1980s-1990s, when scientific problems were demanding more memory<br>\n",
    "   - Idea was made to consider the memory of several interconnected compute nodes as one, known now as <b>distributed memory</b><br>\n",
    "<br>\n",
    "</details>\n",
    "\n",
    "<center><img src=\"../img/DistributedMemory.png\" alt=\"Drawing\" style=\"width: 450px;\"/>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary markdown=\"span\"><b>What is Distributed Memory?</b></summary>\n",
    "<br>\n",
    "   - A set of processors that use their own local memory during computation<br>\n",
    "   - These each exchange data through communications by sending and receiving messages<br>\n",
    "   - Cooperative operations are needed by each process (if one sends, another receives)<br>\n",
    "   - The program is launched as separate processes <b>tasks</b> each with their own address space<br>\n",
    "   - Created in 1980s-1990s, when scientific problems were demanding more memory<br>\n",
    "   - Idea was made to consider the memory of several interconnected compute nodes as one, known now as <b>distributed memory</b><br>\n",
    "<br>\n",
    "</details>\n",
    "\n",
    "<br>\n",
    "\n",
    "As architecture trends changed, shared memory systems were combined over networks creating hybrid distributed memory / shared memory systems. MPI implementors adapted their libraries to handle both types of underlying memory architectures seamlessly. They also adapted/developed ways of handling different interconnects and protocols.\n",
    "\n",
    "Today, MPI runs on virtually any hardware platform:\n",
    "* Distributed Memory\n",
    "* Shared Memory\n",
    "* Hybrid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Execution Model\n",
    "\n",
    "<details>\n",
    "    <summary markdown=\"span\"><b></b></summary>\n",
    "<br>\n",
    "   - In Python, the process model is simpler than with C/Fortran<br>\n",
    "   - Usually a call is needed to initialise MPI using <code>MPI.Init()</code>, but this is done automatically by; <code>from mpi4py import MPI</code>\n",
    "   - The MPI program is launched as a set of independent, identical processes\n",
    "   - These execute the same program code and instructions, and can reside on different nodes/computers\n",
    "    - Ways to launch MPI programs differs depending on the system <code>mpirun</code>, <code>mpiexec</code>, <code>srun</code>...\n",
    "<br>\n",
    "</details>\n",
    "<br>\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>These notebooks are poor for running MPI for anything other than demonstrations. Use provided job scripts to run MPI codes on the compute nodes instead of the login nodes.<b/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting Started\n",
    "\n",
    "To get started with an MPI program you need a **communicator**\n",
    "\n",
    "<details>\n",
    "    <summary markdown=\"span\"><b>What is a communicator?</b></summary>\n",
    "<br>\n",
    "   - A group containing all the processes that will participate in communication<br>\n",
    "   - In <code>mpi4py</code> most MPI calls are implemented as methods of a communicator object<br>\n",
    "   - This can be called using <code>MPI.COMM_WORLD</code><br>\n",
    "   - The user can define custom communicators, which will be covered tomorrow<br>\n",
    "</details>\n",
    "<br>\n",
    "\n",
    "Once you have a communicator, you need a way of identifying all the MPI processes. These are known as **ranks**.\n",
    "\n",
    "\n",
    "<details>\n",
    "    <summary markdown=\"span\"><b>What is a rank?</b></summary>\n",
    "<br>\n",
    "   - A logical ID number given to a process<br>\n",
    "   - A way to query the rank<br>\n",
    "   - Processes can perform different tasks based on their rank.\n",
    "<br>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "if (rank == 0):\n",
    "    # do something\n",
    "elif (rank == 1):\n",
    "    # do something else\n",
    "else:\n",
    "    # all other processes do something else\n",
    "```\n",
    "\n",
    "Aside from the rank, the number of processes also needs to be known. This is called the **size**, and is specified at runtime.\n",
    "\n",
    "After importing MPI, the beginning of any MPI program will have the following three instructions.\n",
    "\n",
    "```python\n",
    "comm = MPI.COMM_WORLD \n",
    "size = comm.Get_size()\n",
    "rank = comm.Get_rank()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary markdown=\"span\"><b>Common Routines in MPI for Python</b></summary>\n",
    "<br>\n",
    "   - Communication between processes, sending and receiving messages between 2 or more processes<br>\n",
    "   - Synchronization between processes<br>\n",
    "   - Communicator creation and manipulation<br>\n",
    "   - Advanced features (e.g. user defined datatypes, one-sided communication and parallel I/O)<br>\n",
    "<br>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## <center><b>Running a Simple MPI Program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile process_greeting.py\n",
    "\n",
    "from mpi4py import MPI\n",
    "\n",
    "# communicator containing all processes\n",
    "comm = MPI.COMM_WORLD \n",
    "\n",
    "size = comm.Get_size()\n",
    "rank = comm.Get_rank()\n",
    "\n",
    "print(\"I am rank %d in group of %d processes\" % (rank, size))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running MPI\n",
    "\n",
    "On the login node;\n",
    "```bash\n",
    "$ mpirun -np 4 python3 hello.py\n",
    "\n",
    "```\n",
    "\n",
    "On the compute node;\n",
    "- Put the above line in a bash file\n",
    "- Submit the bash file to the compute node\n",
    "\n",
    "```bash\n",
    "$ sbatch my_mpi_job.sh\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mpirun -np 4 python3 process_greeting.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <center> [Exercise 1](./05-Exercises-MPI.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## <center><b>Point to Point Communication</b></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary markdown=\"span\"><b>How does Point to Point communication work?</b></summary>\n",
    "<br>\n",
    "    - MPI communicates through <b>messages</b>, which are a number of elements of a particular datatype. These can be basic or special MPI derived datatypes<br>\n",
    "   - Point to Point communication is the communication between two processes, where a source (<code>src</code>) <span style=\"color:red\"><b>sends</b></span> a message to a destination (<code>dest</code>) process which has to <span style=\"color:blue\"><b>receive</b></span> it<br>\n",
    "   - This communication takes place within a communicator, e.g. <code>MPI.COMM_WORLD</code><br>\n",
    "   - Each process in a communicator is identified by their ranks in the communicator<br>\n",
    "   - Sends and receives in a program should match, one <span style=\"color:blue\"><b>receive</b></span> per <span style=\"color:red\"><b>send</b></span>\n",
    "</details>\n",
    "<br>\n",
    "\n",
    "<center><img src=\"../img/MPI_SendRecv.png\" alt=\"Drawing\" style=\"width: 600px;\"/>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "* Two main types:\n",
    "    * synchronous send - sender gets info that message is received\n",
    "    * buffered/asynchronous send - sender knows that the message has left\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sending and Receiving Data\n",
    "\n",
    "Here we will be sending and receiving a dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile send_receive.py\n",
    "\n",
    "from mpi4py import MPI\n",
    "\n",
    "# communicator containing all processes\n",
    "comm = MPI.COMM_WORLD \n",
    "rank = comm.Get_rank()\n",
    "\n",
    "if rank == 0:\n",
    "    data = {'a': 7, 'b': 3.14}\n",
    "    comm.send(data, dest=1)\n",
    "    print('rank ', rank, ' sent: ', data)\n",
    "elif rank == 1:\n",
    "    data = comm.recv(source=0)\n",
    "    print('rank ', rank, ' received: ', data)\n",
    "else:\n",
    "    print('rank ', rank, ' did not receive data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mpirun -np 4 python3 send_receive.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Arbitrary Python objects can be communicated with the <span style=\"color:red\">**send**</span> and <span style=\"color:blue\">**receive**</span> methods of a communicator.\n",
    "* `send(data, dest)`\n",
    "    - `data` - Python object to send\n",
    "    - `dest`  - Destination rank\n",
    "* `recv(source)`\n",
    "    - `source` - source rank\n",
    "    - Data is provided as return value\n",
    "* Destination and source ranks have to match!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Blocking Routines & Deadlocks\n",
    "\n",
    "* `send()` and `recv()` are <span style=\"color:red\">**blocking**</span> routines\n",
    "  - The functions exit only once it is safe to use the data (memory) involved in the communication.\n",
    "* Completion depends on other processes => risk for **_deadlocks_**.\n",
    "  - For example, if all processes call `send()` and there is no-one left to call a corresponding `recv()` then the program is **stuck forever**.\n",
    "  - This will be discussed more in non-blocking communication\n",
    "\n",
    "<center><img src=\"../img/deadlock.png\" alt=\"Drawing\" style=\"width: 300px;\"/></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Typical Point-to-Point communication patterns include pariwise exchanges, where processes talk to their neighbours. The incorrect ordering of sends and receives can result in a deadlock.\n",
    "\n",
    "<center><img src=\"../img/3.2.3.png\" alt=\"Drawing\" style=\"width: 450px;\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <center> [Exercise 2](./05-Exercises-MPI.ipynb)\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <center><b>Case study: Parallel Sum</b></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Array originally on process #0 ($P_{0}$).\n",
    "* Parallel algorithm\n",
    "* **Scatter**\n",
    "  - Half of the array is sent to process 1.\n",
    "* **Compute**\n",
    "  - $P_{0}$ & $P_{1}$ sum independently their segments.\n",
    "* Reduction\n",
    "  - Partial sum on $P_{1}$ sent to $P_{0}$\n",
    "  - $P_{0}$ sums the partial sums.\n",
    "\n",
    "<center><img src=\"../img/3.2.4.png\" alt=\"Drawing\" style=\"width: 150px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 1.1**: Receive operation in scatter\n",
    "\n",
    "<center><img src=\"../img/3.2.4.2.png\" alt=\"Drawing\" style=\"width: 400px;\"/></center>\n",
    "\n",
    "$P_{1}$ posts a <span style=\"color:blue\">**receive**</span> to <span style=\"color:blue\">**receive**</span> *half* of the array **FROM** $P_{0}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 1.2**: Send operation in scatter\n",
    "\n",
    "<center><img src=\"../img/3.2.4.3.png\" alt=\"Drawing\" style=\"width: 400px;\"/></center>\n",
    "\n",
    "$P_{0}$ posts a <span style=\"color:red\">**send**</span> to <span style=\"color:red\">**send**</span> the lower part of the array **TO** $P_{1}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 2**: Compute the sum in parallel\n",
    "\n",
    "<center><img src=\"../img/3.2.4.4.png\" alt=\"Drawing\" style=\"width: 400px;\"/></center>\n",
    "\n",
    "$P_{0}$ & $P_{1}$ computes their parallel sums and stores them locally"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 3.1**: <span style=\"color:blue\">**Receive**</span> operation in reduction\n",
    "\n",
    "<center><img src=\"../img/3.2.4.5.png\" alt=\"Drawing\" style=\"width: 400px;\"/></center>\n",
    "\n",
    "$P_{0}$ posts a <span style=\"color:blue\">**receive**</span> to <span style=\"color:blue\">**receive**</span> partial sum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 3.2**: <span style=\"color:red\">**Send**</span> operation in reduction\n",
    "\n",
    "<center><img src=\"../img/3.2.4.6.png\" alt=\"Drawing\" style=\"width: 400px;\"/></center>\n",
    "\n",
    "$P_{1}$ posts a <span style=\"color:red\">**send**</span> to <span style=\"color:red\">**send**</span> partial sum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 4**: Compute the final answer.\n",
    "\n",
    "<center><img src=\"../img/3.2.4.7.png\" alt=\"Drawing\" style=\"width: 400px;\"/></center>\n",
    "\n",
    "$P_{0}$ sums the partial sums"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## <center><b>Communicating NumPy Arrays</b></center>\n",
    "\n",
    "* Arbitrary Python objects are converted to byte streams (pickled) when sending and back to Python objects (unpickled) when receiving.\n",
    "    - These conversions may be a serious overhead to communication.\n",
    "* Contiguous memory buffers (such as NumPy arrays) can be communicated with very little overhead using upper case methods:\n",
    "    - **`Send(data, dest)`**\n",
    "    - **`Recv(data, source)`**\n",
    "    - Note the difference in receiving: the data array has to exist at the time of call."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Send/Receive a NumPy Array\n",
    "\n",
    "* Note the difference between upper/lower case!\n",
    "* `send`/`recv`: general Python objects --> slow\n",
    "* `Send`/`Recv`: continuous arrays --> fast\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile Send_Recv.py\n",
    "\n",
    "from mpi4py import MPI\n",
    "import numpy as np\n",
    "\n",
    "comm = MPI.COMM_WORLD\n",
    "rank = comm.Get_rank()\n",
    "\n",
    "if rank==0:\n",
    "    data = np.arange(100, dtype=float)\n",
    "    comm.Send(data, dest=1)\n",
    "    print('rank ', rank, ' sent: ', data)\n",
    "elif rank==1:\n",
    "    data = np.empty(100, dtype=float)\n",
    "    comm.Recv(data, source=0)\n",
    "    print('rank ', rank, ' received: ', data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mpirun -np 2 python3 Send_Recv.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <center> [Exercise 3](./05-Exercises-MPI.ipynb)\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <center><b>Combined Send and Receive</b></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* <span style=\"color:red\">Send</span> one message and <span style=\"color:blue\">receive</span> another with a single command\n",
    "    - reduces the risk of deadlocks\n",
    "* Destination and source ranks can be the same or different\n",
    "    - **`MPI.PROC_NULL`** can be used for *no destination/source*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile SendRecv.py\n",
    "\n",
    "from mpi4py import MPI\n",
    "import numpy as np\n",
    "\n",
    "comm = MPI.COMM_WORLD\n",
    "rank = comm.Get_rank()\n",
    "\n",
    "# Send buffer\n",
    "data = np.arange(10, dtype=float) * (rank + 1)\n",
    "\n",
    "# Receive buffer\n",
    "buffer = np.empty(10, float)\n",
    "\n",
    "if rank==0:\n",
    "    dest, source = 1, 1\n",
    "elif rank==1:\n",
    "    dest, source = 0, 0\n",
    "    \n",
    "print('rank ', rank, ' send buffer: ', data)\n",
    "    \n",
    "comm.Sendrecv(data, dest=dest, recvbuf=buffer, source=source)\n",
    "\n",
    "print('rank ', rank, ' receive buffer: ', buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mpirun -np 2 python3 SendRecv.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <center> [Exercise 4](./05-Exercises-MPI.ipynb)\n",
    "***"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
