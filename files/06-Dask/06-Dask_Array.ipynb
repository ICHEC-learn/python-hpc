{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"../../fig/ICHEC_Logo.jpg\" alt=\"Drawing\" style=\"width: 500px;\"/>\n",
    "\n",
    "# <center>Dask<center/>\n",
    "******\n",
    "***\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- Dask allows us to create multiple tasks with a minimum of effort.\n",
    "\n",
    "- There are different apects to Dask of which we will cover just a few.\n",
    "\n",
    "- Dask is integrated with different projects such as pandas, numpy and scipy.\n",
    "\n",
    "- This allows parallelisation of say scipy functions with minimum changes to the python code.\n",
    "\n",
    "- Some of the projects have their own multithreaded functions, so dask is not essential."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dask Distributed\n",
    "\n",
    "- The first step is to create a set of workers with a scheduler.\n",
    "\n",
    "- Here we are creating 2 worker processes with 2 threads each.\n",
    "\n",
    "- There will be 1GB for all workers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask\n",
    "from dask.distributed import Client\n",
    "\n",
    "client = Client(n_workers=2, threads_per_worker=2, memory_limit='1GB')\n",
    "\n",
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- This creates a LocalCluster in Dask.\n",
    "\n",
    "- This means that all the workers will be on a single node.\n",
    "\n",
    "- You can see there is a link to the scheduler.\n",
    "\n",
    "- Unfortunately if you click on the link you cannot access the page.\n",
    "\n",
    "- There are a couple of ways around this, create another tunnel to port 8787.\n",
    "\n",
    "- Or install jupyter-server-proxy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Here we have a simple example where we estimate $\\pi$.\n",
    "\n",
    "- This version uses numpy functions to perform the calculations.\n",
    "\n",
    "- Note there are no explicit loops.\n",
    "\n",
    "- The code is serial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_pi(size_in_bytes):\n",
    "    \n",
    "    \"\"\"Calculate pi using a Monte Carlo method.\"\"\"\n",
    "    \n",
    "    rand_array_shape = (int(size_in_bytes / 8 / 2), 2)\n",
    "    \n",
    "    # 2D random array with positions (x, y)\n",
    "    xy = np.random.uniform(low=0.0, high=1.0, size=rand_array_shape)\n",
    "    \n",
    "    # check if position (x, y) is in unit circle\n",
    "    xy_inside_circle = (xy ** 2).sum(axis=1) < 1\n",
    "\n",
    "    # pi is the fraction of points in circle x 4\n",
    "    pi = 4 * xy_inside_circle.sum() / xy_inside_circle.size\n",
    "\n",
    "    print(f\"\\nfrom {xy.nbytes / 1e9} GB randomly chosen positions\")\n",
    "    print(f\"   pi estimate: {pi}\")\n",
    "    print(f\"   pi error: {abs(pi - np.pi)}\\n\")\n",
    "    \n",
    "    return pi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time calculate_pi(10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Without needing to change the function we can use Dask to parallelise the work.\n",
    "\n",
    "- In this case we have two workers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = Client(n_workers=2, threads_per_worker=2, memory_limit='1GB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dask_calpi = dask.delayed(calculate_pi)(10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time dask.compute(dask_calpi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.restart()\n",
    "client.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- But wait it is taking longer with Dask!\n",
    "\n",
    "- You can change the size of the problem but still find that Dask is slower.\n",
    "\n",
    "- This is because we are using Dask incorrectly, the function cannot be divided into tasks.\n",
    "\n",
    "- We can visualise how the tasks are decomposed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dask.visualize(dask_calpi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We can only get task parallelism if we call the function more than once.\n",
    "\n",
    "- Below is an example of this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "for i in range(5):\n",
    "    dask_calpi = dask.delayed(calculate_pi)(10000*(i+1))\n",
    "    results.append(dask_calpi)\n",
    "    \n",
    "dask.visualize(results)\n",
    "\n",
    "#to run all the tasks use\n",
    "#dask.compute(*results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Exercise 1](exercises/06-Dask_Array_Exercise1.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dask Array\n",
    "\n",
    "- From the previous example parallelisation comes with a downside.\n",
    "\n",
    "- If we have 1 task that needs 10GB memory, having 10 simultanteously will need 100GB in total.\n",
    "\n",
    "- Each kay node has 180GB of memory, what if we need more than that?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Dask Array splits a larger array into smaller chunks.\n",
    "\n",
    "- We can work on larger than memory arrays but still use all of the cores.\n",
    "\n",
    "- You can think of the Dask array as a set of smaller numpy arrays.\n",
    "\n",
    "<img src=\"../../fig/notebooks/dask-array-black-text.svg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Below we have some simple examples of using dask arrays\n",
    "\n",
    "- Again we need to setup a LocalCluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = Client(n_workers=2, threads_per_worker=1, memory_limit='1GB')\n",
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.array as da"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*These examples courtesy of Dask contributor James Bourbeau*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_np = np.arange(1, 50, 3)   # Create array with numbers 1->50 but with a stride of 3\n",
    "print(a_np)\n",
    "a_np.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_da = da.arange(1, 50, 3, chunks=5)\n",
    "a_da"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Notice that we split the 1D array into 4 chunks with a maximum size of 5 elements.\n",
    "\n",
    "- Also that dask is smart enough to have setup a set of tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(a_da.dtype)\n",
    "print(a_da.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(a_da.chunks)\n",
    "print(a_da.chunksize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_da.visualize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Four tasks have been created for each of the subarrays.\n",
    "\n",
    "- Take an operation where the tasks are independent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(a_da ** 2).visualize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Up to now we have omly been setting up the task graph or how the workload is split.\n",
    "\n",
    "- To actually perform the set of tasks we need to *compute* the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(a_da ** 2).compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- You can see that the parallelisation is very simple.\n",
    "\n",
    "- A numpy array is returned.\n",
    "\n",
    "- There are other operations that can be done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type((a_da ** 2).compute())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dask arrays support a large portion of the NumPy interface:\n",
    "\n",
    "- Arithmetic and scalar mathematics: `+`, `*`, `exp`, `log`, ...\n",
    "\n",
    "- Reductions along axes: `sum()`, `mean()`, `std()`, `sum(axis=0)`, ...\n",
    "\n",
    "- Tensor contractions / dot products / matrix multiply: `tensordot`\n",
    "\n",
    "- Axis reordering / transpose: `transpose`\n",
    "\n",
    "- Slicing: `x[:100, 500:100:-2]`\n",
    "\n",
    "- Fancy indexing along single axes with lists or numpy arrays: `x[:, [10, 1, 5]]`\n",
    "\n",
    "- Array protocols like `__array__` and `__array_ufunc__`\n",
    "\n",
    "- Some linear algebra: `svd`, `qr`, `solve`, `solve_triangular`, `lstsq`, ...\n",
    "\n",
    "- ...\n",
    "\n",
    "See the [Dask array API docs](http://docs.dask.org/en/latest/array-api.html) for full details about what portion of the NumPy API is implemented for Dask arrays."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Blocked Algorithms\n",
    "\n",
    "- Dask arrays are implemented using _blocked algorithms_. \n",
    "\n",
    "- These algorithms break up a computation on a large array into many computations on smaller pieces of the array. \n",
    "\n",
    "- This minimizes the memory load (amount of RAM) of computations and allows for working with larger-than-memory datasets in parallel.\n",
    "\n",
    "- Dask supports a large protion of the numpy functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = da.random.random(20, chunks=5)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "result = x.sum()\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Here we have a reduction operation.\n",
    "\n",
    "- The results of each task are accumulated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.visualize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- Here we have a much more complicated operation.\n",
    "\n",
    "- Because we are taking the tranpose of a matrix, there are many more tasks.\n",
    "\n",
    "- Again this is handled seamlessly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "x = da.random.random(size=(15, 15), chunks=(10, 5))\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "x.chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "result = (x + x.T).sum()\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.visualize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can perform computations on larger-than-memory arrays!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.restart()\n",
    "client.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate Pi\n",
    "\n",
    "- Going back to the previous example, calculating $\\pi$.\n",
    "\n",
    "- We could use dask arrays rather than numpy.\n",
    "\n",
    "- We can parallelise the function itself and tackle larger problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dask_calculate_pi(size_in_bytes,nchunks):\n",
    "    \n",
    "    \"\"\"Calculate pi using a Monte Carlo method.\"\"\"\n",
    "    \n",
    "    rand_array_shape = (int(size_in_bytes / 8 / 2), 2)\n",
    "    chunk_size = int(rand_array_shape[0]/nchunks)\n",
    "    \n",
    "    # 2D random array with positions (x, y)\n",
    "    xy = da.random.uniform(low=0.0, high=1.0, size=rand_array_shape, chunks=chunk_size)\n",
    "    print(xy)\n",
    "    \n",
    "    # check if position (x, y) is in unit circle\n",
    "    xy_inside_circle = (xy ** 2).sum(axis=1) < 1\n",
    "\n",
    "    # pi is the fraction of points in circle x 4\n",
    "    pi = 4 * xy_inside_circle.sum() / xy_inside_circle.size\n",
    "    \n",
    "    result = pi.compute()\n",
    "\n",
    "    print(f\"\\nfrom {xy.nbytes / 1e9} GB randomly chosen positions\")\n",
    "    print(f\"   pi estimate: {result}\")\n",
    "    print(f\"   pi error: {abs(result - np.pi)}\\n\")\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = Client(n_workers=5,threads_per_worker=1,memory_limit='1GB')\n",
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time dask_calculate_pi(10000000,20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Exercise 2](exercises/06-Dask_Array_Exercise2.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary\n",
    "\n",
    "- Dask (distributed) allows task parallelization of problems.\n",
    "\n",
    "- The changes to the code base are minimal but you do need to identify the tasks.\n",
    "\n",
    "- A scheduler is created to allocate the tasks to the workers.\n",
    "\n",
    "- Dask-array is a method to create a set of tasks automatically through operations on arrays.\n",
    "\n",
    "- Dask-array sits on top of dask.\n",
    "\n",
    "- Tasks are created by splitting a larger array(s), which means that larger than memory problems can be handled."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Links\n",
    "\n",
    "__[Dask Home Page](http://dask.org)__\n",
    "\n",
    "__[Dask Distributed](https://distributed.dask.org/en/stable/)__\n",
    "\n",
    "__[Dask Array](https://docs.dask.org/en/stable/array.html)__"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
