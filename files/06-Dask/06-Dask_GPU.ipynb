{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPU Basics.\n",
    "\n",
    "- GPGPU is a device that is external to the machine CPU but can perform computation.\n",
    "\n",
    "- The benefits of using a GPU is that it has many more cores than a CPU which are more efficient at particular operations.\n",
    "\n",
    "- Kay has Tesla VT100 which have 5120 CUDA cores.\n",
    "\n",
    "- These cores are arranged in streaming multiprocessors (SM) of which there are 80.\n",
    "\n",
    "- The downsides are that GPU cores not not as flexible as CPU cores.\n",
    "\n",
    "- Data needs to be moved between CPU and GPU memory, increasing overheads.\n",
    "\n",
    "- The memory per core is tiny compared to that of the CPU.\n",
    "\n",
    "- GPGPUs are best when performing SIMD calculations, due to the fact that groups of cores can omly perform a single instruction at any one time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Tesla VT100](../img/tesla-vt100.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CUDA Basics\n",
    "\n",
    "- CUDA is not a language in itself but are extensions to C.\n",
    "\n",
    "- There is a single source code which defnes which computation is done on the CPU and which on the GPU.\n",
    "\n",
    "- The CPU controls the flow of the execution.\n",
    "\n",
    "- The CPU is called the host and the GPU the device.\n",
    "\n",
    "- The host runs C functions defined the same way as normal.\n",
    "\n",
    "- The device runs what are called kernels which are similar to C functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The execution model is thread based similar to OpenMP.\n",
    "\n",
    "- Each kernel has a grid and grids are organised into blocks.\n",
    "\n",
    "- Each block has a number of threads.\n",
    "\n",
    "- One block is executed on a single SM, so there is a maximum number of threads a block can have.\n",
    "\n",
    "- These constructions can be 1D,2D, or 3D.\n",
    "\n",
    "![Diagram of Grid/Block](../img/cuda3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring your GPU device(s).\n",
    "\n",
    "- For each GPU node on Kay there are 2 Tesla VT100s which have 16GB memory each.\n",
    "\n",
    "- One way to access the GPUs from python is by using the package pyCUDA.\n",
    "\n",
    "- Through this we can examine them.\n",
    "\n",
    "- The login nodes do not have GPUs, so we need to submit the script below to the GpuQ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile gpu_test.py\n",
    "import pycuda.driver as drv\n",
    "\n",
    "drv.init()\n",
    "drv.get_version()\n",
    "\n",
    "devn = drv.Device.count()\n",
    "print ('Localized GPUs =',devn)\n",
    "\n",
    "\n",
    "sp = drv.Device(0)\n",
    "\n",
    "print ('Name = ',sp.name())\n",
    "print ('PCI Bus = ',sp.pci_bus_id())\n",
    "print ('Compute Capability = ',sp.compute_capability())\n",
    "print ('Total Memory = ',sp.total_memory()/(2.**20) , 'MBytes')\n",
    "attr = sp.get_attributes()\n",
    "for j in range(len(attr.items())):\n",
    "    print (list(attr.items())[j])#,'Bytes (when apply)'\n",
    "print ('------------------')\n",
    "print ('------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile gpu_test.slurm\n",
    "#!/bin/bash\n",
    "#SBATCH --nodes=1\n",
    "#SBATCH --time=00:10:00\n",
    "#SBATCH -A course\n",
    "#SBATCH --job-name=test\n",
    "#SBATCH -p GpuQ\n",
    "#SBATCH --reservation=May_Course_GPU\n",
    "\n",
    "module purge\n",
    "module load conda cuda/11.4 gcc/8.2.0\n",
    "module list\n",
    "\n",
    "source activate /ichec/home/users/course00/conda_HPC\n",
    "\n",
    "cd $SLURM_SUBMIT_DIR\n",
    "\n",
    "\n",
    "python -u gpu_test.py\n",
    "\n",
    "\n",
    "\n",
    "exit 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Below is the output from this script.\n",
    "\n",
    "- A few of the items are highlighted with \"***\"\n",
    "\n",
    "- You can see the maximum block and grid sizes.\n",
    "\n",
    "- Notice that there is a maximum number of threads per block.\n",
    "\n",
    "- At the bottom you can see the wrap size.\n",
    "\n",
    "- Threads in a wrap are constrained to run the same instruction/operation at any one time.\n",
    "\n",
    "- Block sizes should be a multiple of the wrap size."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "Name =  Tesla V100-PCIE-16GB\n",
    "PCI Bus =  0000:5E:00.0\n",
    "Compute Capability =  (7, 0)\n",
    "Total Memory =  16130.5 MBytes\n",
    "(pycuda._driver.device_attribute.ASYNC_ENGINE_COUNT, 7)\n",
    "(pycuda._driver.device_attribute.CAN_MAP_HOST_MEMORY, 1)\n",
    "(pycuda._driver.device_attribute.CLOCK_RATE, 1380000)\n",
    "(pycuda._driver.device_attribute.COMPUTE_CAPABILITY_MAJOR, 7)\n",
    "(pycuda._driver.device_attribute.COMPUTE_CAPABILITY_MINOR, 0)\n",
    "(pycuda._driver.device_attribute.COMPUTE_MODE, pycuda._driver.compute_mode.DEFAULT)\n",
    "(pycuda._driver.device_attribute.CONCURRENT_KERNELS, 1)\n",
    "(pycuda._driver.device_attribute.ECC_ENABLED, 1)\n",
    "(pycuda._driver.device_attribute.GLOBAL_L1_CACHE_SUPPORTED, 1)\n",
    "(pycuda._driver.device_attribute.GLOBAL_MEMORY_BUS_WIDTH, 4096)\n",
    "(pycuda._driver.device_attribute.GPU_OVERLAP, 1)\n",
    "(pycuda._driver.device_attribute.INTEGRATED, 0)\n",
    "(pycuda._driver.device_attribute.KERNEL_EXEC_TIMEOUT, 0)\n",
    "(pycuda._driver.device_attribute.L2_CACHE_SIZE, 6291456)\n",
    "(pycuda._driver.device_attribute.LOCAL_L1_CACHE_SUPPORTED, 1)\n",
    "(pycuda._driver.device_attribute.MANAGED_MEMORY, 1)\n",
    "(pycuda._driver.device_attribute.MAXIMUM_SURFACE1D_LAYERED_LAYERS, 2048)\n",
    "(pycuda._driver.device_attribute.MAXIMUM_SURFACE1D_LAYERED_WIDTH, 32768)\n",
    "(pycuda._driver.device_attribute.MAXIMUM_SURFACE1D_WIDTH, 32768)\n",
    "(pycuda._driver.device_attribute.MAXIMUM_SURFACE2D_HEIGHT, 65536)\n",
    "(pycuda._driver.device_attribute.MAXIMUM_SURFACE2D_LAYERED_HEIGHT, 32768)\n",
    "(pycuda._driver.device_attribute.MAXIMUM_SURFACE2D_LAYERED_LAYERS, 2048)\n",
    "(pycuda._driver.device_attribute.MAXIMUM_SURFACE2D_LAYERED_WIDTH, 32768)\n",
    "(pycuda._driver.device_attribute.MAXIMUM_SURFACE2D_WIDTH, 131072)\n",
    "(pycuda._driver.device_attribute.MAXIMUM_SURFACE3D_DEPTH, 16384)\n",
    "(pycuda._driver.device_attribute.MAXIMUM_SURFACE3D_HEIGHT, 16384)\n",
    "(pycuda._driver.device_attribute.MAXIMUM_SURFACE3D_WIDTH, 16384)\n",
    "(pycuda._driver.device_attribute.MAXIMUM_SURFACECUBEMAP_LAYERED_LAYERS, 2046)\n",
    "(pycuda._driver.device_attribute.MAXIMUM_SURFACECUBEMAP_LAYERED_WIDTH, 32768)\n",
    "(pycuda._driver.device_attribute.MAXIMUM_SURFACECUBEMAP_WIDTH, 32768)\n",
    "(pycuda._driver.device_attribute.MAXIMUM_TEXTURE1D_LAYERED_LAYERS, 2048)\n",
    "(pycuda._driver.device_attribute.MAXIMUM_TEXTURE1D_LAYERED_WIDTH, 32768)\n",
    "(pycuda._driver.device_attribute.MAXIMUM_TEXTURE1D_LINEAR_WIDTH, 134217728)\n",
    "(pycuda._driver.device_attribute.MAXIMUM_TEXTURE1D_MIPMAPPED_WIDTH, 32768)\n",
    "(pycuda._driver.device_attribute.MAXIMUM_TEXTURE1D_WIDTH, 131072)\n",
    "(pycuda._driver.device_attribute.MAXIMUM_TEXTURE2D_ARRAY_HEIGHT, 32768)\n",
    "(pycuda._driver.device_attribute.MAXIMUM_TEXTURE2D_ARRAY_NUMSLICES, 2048)\n",
    "(pycuda._driver.device_attribute.MAXIMUM_TEXTURE2D_ARRAY_WIDTH, 32768)\n",
    "(pycuda._driver.device_attribute.MAXIMUM_TEXTURE2D_GATHER_HEIGHT, 32768)\n",
    "(pycuda._driver.device_attribute.MAXIMUM_TEXTURE2D_GATHER_WIDTH, 32768)\n",
    "(pycuda._driver.device_attribute.MAXIMUM_TEXTURE2D_HEIGHT, 65536)\n",
    "(pycuda._driver.device_attribute.MAXIMUM_TEXTURE2D_LINEAR_HEIGHT, 65000)\n",
    "(pycuda._driver.device_attribute.MAXIMUM_TEXTURE2D_LINEAR_PITCH, 2097120)\n",
    "(pycuda._driver.device_attribute.MAXIMUM_TEXTURE2D_LINEAR_WIDTH, 131072)\n",
    "(pycuda._driver.device_attribute.MAXIMUM_TEXTURE2D_MIPMAPPED_HEIGHT, 32768)\n",
    "(pycuda._driver.device_attribute.MAXIMUM_TEXTURE2D_MIPMAPPED_WIDTH, 32768)\n",
    "(pycuda._driver.device_attribute.MAXIMUM_TEXTURE2D_WIDTH, 131072)\n",
    "(pycuda._driver.device_attribute.MAXIMUM_TEXTURE3D_DEPTH, 16384)\n",
    "(pycuda._driver.device_attribute.MAXIMUM_TEXTURE3D_DEPTH_ALTERNATE, 32768)\n",
    "(pycuda._driver.device_attribute.MAXIMUM_TEXTURE3D_HEIGHT, 16384)\n",
    "(pycuda._driver.device_attribute.MAXIMUM_TEXTURE3D_HEIGHT_ALTERNATE, 8192)\n",
    "(pycuda._driver.device_attribute.MAXIMUM_TEXTURE3D_WIDTH, 16384)\n",
    "(pycuda._driver.device_attribute.MAXIMUM_TEXTURE3D_WIDTH_ALTERNATE, 8192)\n",
    "(pycuda._driver.device_attribute.MAXIMUM_TEXTURECUBEMAP_LAYERED_LAYERS, 2046)\n",
    "(pycuda._driver.device_attribute.MAXIMUM_TEXTURECUBEMAP_LAYERED_WIDTH, 32768)\n",
    "(pycuda._driver.device_attribute.MAXIMUM_TEXTURECUBEMAP_WIDTH, 32768)\n",
    "(pycuda._driver.device_attribute.MAX_BLOCK_DIM_X, 1024)                          ******\n",
    "(pycuda._driver.device_attribute.MAX_BLOCK_DIM_Y, 1024)                          ******\n",
    "(pycuda._driver.device_attribute.MAX_BLOCK_DIM_Z, 64)                            ******\n",
    "(pycuda._driver.device_attribute.MAX_GRID_DIM_X, 2147483647)                     ******\n",
    "(pycuda._driver.device_attribute.MAX_GRID_DIM_Y, 65535)                          ******\n",
    "(pycuda._driver.device_attribute.MAX_GRID_DIM_Z, 65535)                          ******\n",
    "(pycuda._driver.device_attribute.MAX_PITCH, 2147483647)\n",
    "(pycuda._driver.device_attribute.MAX_REGISTERS_PER_BLOCK, 65536)\n",
    "(pycuda._driver.device_attribute.MAX_REGISTERS_PER_MULTIPROCESSOR, 65536)\n",
    "(pycuda._driver.device_attribute.MAX_SHARED_MEMORY_PER_BLOCK, 49152)\n",
    "(pycuda._driver.device_attribute.MAX_SHARED_MEMORY_PER_MULTIPROCESSOR, 98304)\n",
    "(pycuda._driver.device_attribute.MAX_THREADS_PER_BLOCK, 1024)                    ******\n",
    "(pycuda._driver.device_attribute.MAX_THREADS_PER_MULTIPROCESSOR, 2048)\n",
    "(pycuda._driver.device_attribute.MEMORY_CLOCK_RATE, 877000)\n",
    "(pycuda._driver.device_attribute.MULTIPROCESSOR_COUNT, 80)\n",
    "(pycuda._driver.device_attribute.MULTI_GPU_BOARD, 0)\n",
    "(pycuda._driver.device_attribute.MULTI_GPU_BOARD_GROUP_ID, 0)\n",
    "(pycuda._driver.device_attribute.PCI_BUS_ID, 94)\n",
    "(pycuda._driver.device_attribute.PCI_DEVICE_ID, 0)\n",
    "(pycuda._driver.device_attribute.PCI_DOMAIN_ID, 0)\n",
    "(pycuda._driver.device_attribute.STREAM_PRIORITIES_SUPPORTED, 1)\n",
    "(pycuda._driver.device_attribute.SURFACE_ALIGNMENT, 512)\n",
    "(pycuda._driver.device_attribute.TCC_DRIVER, 0)\n",
    "(pycuda._driver.device_attribute.TEXTURE_ALIGNMENT, 512)\n",
    "(pycuda._driver.device_attribute.TEXTURE_PITCH_ALIGNMENT, 32)\n",
    "(pycuda._driver.device_attribute.TOTAL_CONSTANT_MEMORY, 65536)\n",
    "(pycuda._driver.device_attribute.UNIFIED_ADDRESSING, 1)\n",
    "(pycuda._driver.device_attribute.WARP_SIZE, 32)                                  ******\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| <img src=\"../img/Terminalicon2.png\" height=100 width=100>|\n",
    "|:--:|\n",
    "| gpu_test |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vector Addition\n",
    "\n",
    "- A typical example of using a GPU to perform work is vector addition.\n",
    "\n",
    "- This is a trivially parallelizable example because the operation for each element of the array is independent.\n",
    "\n",
    "- Below is the normal way of doing this in C.\n",
    "\n",
    "![Vector Addition](../img/suma.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Version C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile vecadd.c\n",
    "#include <stdio.h>\n",
    "#include <stdlib.h>\n",
    "\n",
    "int main(void)\n",
    "{\n",
    "    int N = 10;\n",
    "    float *a,*b,*c;\n",
    "\n",
    "    // Reserve memory\n",
    "    a = (float *) malloc(N * sizeof(float));\n",
    "    b = (float *) malloc(N * sizeof(float));\n",
    "    c = (float *) malloc(N * sizeof(float));\n",
    "\n",
    "\n",
    "    // Initialize arrays\n",
    "    for (int i = 0; i < N; ++i){\n",
    "        a[i] = i;\n",
    "        b[i] = 2.0f;\t\n",
    "    }\n",
    "\n",
    "\n",
    "    // Perform vector addition\n",
    "    for (int i = 0; i < N; ++i){\n",
    "        c[i]= a[i]+b[i];\t\n",
    "    }\n",
    "\n",
    "    printf(\"Done %f\\n\",c[0]);\n",
    "\n",
    "    // Free arrays\n",
    "    free(a); free(b); free(c);\n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|<img src=\"../img/Terminalicon2.png\" height=100 width=100>|\n",
    "|:--:|\n",
    "| veadd.c |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Version CUDA C\n",
    "\n",
    "- An individual thread is identified through the block ID and the thread ID within the block."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Threads in block](../img/CUDAmodelThreads.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Below is the CUDA version of vector addition.\n",
    "\n",
    "- The vectorAdd fucntion is the CUDA kernel.\n",
    "\n",
    "- Notice that there is no for loop.\n",
    "\n",
    "- It is written as if for a single thread.\n",
    "\n",
    "- A single threads adds a single element, that element is determined by the thread and block IDs.\n",
    "\n",
    "- To compile this code we use `nvcc`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile vecadd.cu\n",
    "#include <stdio.h>\n",
    "#include <stdlib.h>\n",
    "#include <cuda_runtime.h>\n",
    "// CUDA Kernel\n",
    "__global__ void vectorAdd(const float *A, const float *B, float *C, int numElements)\n",
    "{\n",
    "    int i = blockDim.x * blockIdx.x + threadIdx.x;\n",
    "    if (i < numElements)\n",
    "    {\n",
    "        C[i] = A[i] + B[i];\n",
    "    }\n",
    "}\n",
    "\n",
    "/*\n",
    " * Host main routine\n",
    " */\n",
    "int main(void)\n",
    "{\n",
    "    int numElements = 15;\n",
    "    size_t size = numElements * sizeof(float);\n",
    "    printf(\"[Vector addition of %d elements]\\n\", numElements);\n",
    "\n",
    "    float *a,*b,*c;\n",
    "    float *a_gpu,*b_gpu,*c_gpu;\n",
    "\n",
    "    // Reserve host memory\n",
    "    a = (float *) malloc(size);\n",
    "    b = (float *) malloc(size);\n",
    "    c = (float *) malloc(size);\n",
    "    \n",
    "    // Reserve device memory\n",
    "    cudaMalloc((void **)&a_gpu, size);\n",
    "    cudaMalloc((void **)&b_gpu, size);\n",
    "    cudaMalloc((void **)&c_gpu, size);\n",
    "\n",
    "    // Initialize arrays\n",
    "    for (int i=0;i<numElements;++i ){\n",
    "    \ta[i] = i;\n",
    "    \tb[i] = 2.0f;\n",
    "    }\n",
    "    \n",
    "    // Copy the host input vectors A and B in host memory to the device input vectors in\n",
    "    // device memory\n",
    "    printf(\"Copy input data from the host memory to the CUDA device\\n\");\n",
    "    cudaMemcpy(a_gpu, a, size, cudaMemcpyHostToDevice);\n",
    "    cudaMemcpy(b_gpu, b, size, cudaMemcpyHostToDevice);\n",
    "\n",
    "    // Launch the Vector Add CUDA Kernel\n",
    "    int threadsPerBlock = 256;\n",
    "    int blocksPerGrid =(numElements + threadsPerBlock - 1) / threadsPerBlock;\n",
    "    printf(\"CUDA kernel launch with %d blocks of %d threads\\n\", blocksPerGrid, threadsPerBlock);\n",
    "    vectorAdd<<<blocksPerGrid, threadsPerBlock>>>(a_gpu, b_gpu, c_gpu, numElements);\n",
    "\n",
    "    // Copy the device result vector in device memory to the host result vector\n",
    "    // in host memory.\n",
    "    printf(\"Copy output data from the CUDA device to the host memory\\n\");\n",
    "    cudaMemcpy(c, c_gpu, size, cudaMemcpyDeviceToHost);\n",
    "\n",
    "    for (int i=0;i<numElements;++i ){\n",
    "    \tprintf(\"%f \\n\",c[i]);\n",
    "    }\n",
    "\n",
    "    // Free host memory\n",
    "    free(a); free(b); free(c);\n",
    "    \n",
    "    // Free device global memory\n",
    "    cudaFree(a_gpu);\n",
    "    cudaFree(b_gpu);\n",
    "    cudaFree(c_gpu);\n",
    "    \n",
    "    printf(\"Done\\n\");\n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|<img src=\"../img/Terminalicon2.png\" height=100 width=100>|\n",
    "|:--:|\n",
    "| vecadd.cu |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyCUDA\n",
    "\n",
    "- So far we have been using the GPUs through cuda code.\n",
    "\n",
    "- PyCUDA is a framework which allows us to access the GPU form a python environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- There are different ways to achieve this. \n",
    "\n",
    "- First we will look at GPUArrays:\n",
    "\n",
    " - We can copy arrays from the host to device.\n",
    "    \n",
    " - Perform basic operations.\n",
    "    \n",
    " - Return the data to te host."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPUArrays\n",
    "\n",
    "- The code below does the same as those above.\n",
    "\n",
    "- Much of the work is hidden, like python itself.\n",
    "\n",
    "- Also you can view the data both on the CPU and GPU.\n",
    "\n",
    "- The distinction between what is on the GPU and CPU is blurred which may cause problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile gpu_array.py\n",
    "from pycuda import autoinit\n",
    "from pycuda import gpuarray\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Create single precision host arrays\n",
    "aux = range(15)\n",
    "a = np.array(aux).astype(np.float32)\n",
    "b = 2.0*np.ones(len(aux),dtype=np.float32)\n",
    "\n",
    "# Create and copy data to GPU \n",
    "a_gpu = gpuarray.to_gpu(a)\n",
    "b_gpu = gpuarray.to_gpu(b)\n",
    "\n",
    "# Perform operation on GPU\n",
    "aux_gpu = a_gpu+b_gpu\n",
    "\n",
    "# Return data to host\n",
    "c = aux_gpu.get()\n",
    "\n",
    "\n",
    "\n",
    "print(\"a_gpu=\")\n",
    "print(a_gpu)\n",
    "\n",
    "print(\"b_gpu=\")\n",
    "print(b_gpu)\n",
    "\n",
    "print(\"aux_gpu=\")\n",
    "print(type(aux_gpu))\n",
    "print(aux_gpu)\n",
    "\n",
    "print(\"c=\")\n",
    "print(type(c))\n",
    "print(c)\n",
    "\n",
    "\n",
    "# Free memory on GPU\n",
    "del(a_gpu)\n",
    "b_gpu.gpudata.free()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We cannot run this code here we must move to the terminal.\n",
    "\n",
    "|<img src=\"../img/Terminalicon2.png\" height=100 width=100>|\n",
    "|:--:|\n",
    "| gpu_array |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using source code\n",
    "\n",
    "- Similarly to Cython we can use the cuda code we just created.\n",
    "\n",
    "- However notice that we are only using the kernel.\n",
    "\n",
    "- There is more coding to be done than the first approach using gpuarrays only.\n",
    "\n",
    "- It might be necessary for more complex kernels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile gpu_kernel.py\n",
    "from pycuda import autoinit\n",
    "from pycuda import gpuarray\n",
    "import pycuda.driver as drv\n",
    "import numpy as np\n",
    "from pycuda.compiler import SourceModule\n",
    "\n",
    "# Read in source code\n",
    "cudaCode = open(\"vecadd.cu\",\"r\")\n",
    "myCUDACode = cudaCode.read()\n",
    "myCode = SourceModule(myCUDACode)\n",
    "\n",
    "# Extract vectorAdd kernel\n",
    "importedKernel = myCode.get_function(\"vectorAdd\")\n",
    "\n",
    "# Create host arrays\n",
    "aux = range(15)\n",
    "a = np.array(aux).astype(np.float32)\n",
    "b = 2.0*np.ones(len(aux),dtype=np.float32)\n",
    "c = np.zeros(len(aux),dtype=np.float32)\n",
    "\n",
    "# Create and copy data to GPU, need to three arrays as there are three arguments to vectorAdd \n",
    "a_gpu = gpuarray.to_gpu(a)\n",
    "b_gpu = gpuarray.to_gpu(b)\n",
    "c_gpu = gpuarray.to_gpu(c)\n",
    "\n",
    "# Set grid/block properties\n",
    "threadsPerBlock = 256\n",
    "blocksPerGrid = (len(aux) + threadsPerBlock - 1) / threadsPerBlock;\n",
    "\n",
    "\n",
    "# Perform operation\n",
    "# Need to give the number of blocks per grid in 3D\n",
    "# Need to give block size in 3D\n",
    "importedKernel(a_gpu.gpudata,b_gpu.gpudata,c_gpu.gpudata,block=(threadsPerBlock,blocksPerGrid,1),grid=(1,1,1))\n",
    "\n",
    "# Wait for computation to finish\n",
    "drv.Context.synchronize()\n",
    "\n",
    "# \n",
    "c = c_gpu.get()\n",
    "print(c=\")\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|<img src=\"../img/Terminalicon2.png\" height=100 width=100>|\n",
    "|:--:|\n",
    "| gpu_kernel |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Elementwise Kernels\n",
    "\n",
    "- Yet another way is to use a predefined function ElementwiseKernel.\n",
    "\n",
    "- As its name suggests it performs operations that are trivially parallel across the array.\n",
    "\n",
    "- There are other similar pyCUDA functions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile gpu_elem.py\n",
    "from pycuda import autoinit\n",
    "from pycuda import gpuarray\n",
    "import numpy as np\n",
    "from pycuda.elementwise import ElementwiseKernel\n",
    "\n",
    "# Create host arrays\n",
    "aux = range(15)\n",
    "a = np.array(aux).astype(np.float32)\n",
    "b = 2.0*np.ones(len(aux),dtype=np.float32)\n",
    "c = np.zeros(len(aux),dtype=np.float32)\n",
    "\n",
    "# Create and copy data to GPU, need to three arrays as there are three arguments to vectorAdd \n",
    "a_gpu = gpuarray.to_gpu(a)\n",
    "b_gpu = gpuarray.to_gpu(b)\n",
    "c_gpu = gpuarray.to_gpu(c)\n",
    "\n",
    "# Create the function that does vector addition in this case\n",
    "myCudaFunc = ElementwiseKernel(arguments = \"float *a, float *b, float *c\",\n",
    "                               operation = \"c[i] = a[i]+b[i]\",\n",
    "                               name = \"myVecAdd\")\n",
    "# Execute function\n",
    "myCudaFunc(a_gpu,b_gpu,c_gpu)\n",
    "\n",
    "# Return data to host\n",
    "c = c_gpu.get()\n",
    "\n",
    "print(\"c =\")\n",
    "print(c)\n",
    "\n",
    "# Free memory on GPU\n",
    "a_gpu.gpudata.free()\n",
    "b_gpu.gpudata.free()\n",
    "c_gpu.gpudata.free()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DASK GPU\n",
    "\n",
    "- It is in its early stages of development.\n",
    "\n",
    "- I had to make a code change to get it to work on Kay.\n",
    "\n",
    "- It is very easy to setup."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- There is the example we had previously calculating $\\pi$.\n",
    "\n",
    "- The only difference is that we setup a LocalCUDACluster.\n",
    "\n",
    "- This then uses both GPUs as the workers.\n",
    "\n",
    "- Our example we have here is much slower than the CPU version."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- I expect that only certain problems will be accelerated using this approach.\n",
    "\n",
    "- But using Dask array we have been able to work on an array much larger than the GPU memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile dask_GPU_calculate_pi.py\n",
    "import numpy as np\n",
    "\n",
    "import dask\n",
    "import dask.array as da\n",
    "from dask.distributed import Client\n",
    "from dask_cuda import LocalCUDACluster         # Added\n",
    "\n",
    "import time\n",
    "\n",
    "\n",
    "def dask_calculate_pi(size_in_bytes,nchunks):\n",
    "    \n",
    "    \"\"\"Calculate pi using a Monte Carlo method.\"\"\"\n",
    "    \n",
    "    rand_array_shape = (int(size_in_bytes / 8 / 2), 2)\n",
    "    chunk_size = int(rand_array_shape[0]/nchunks)\n",
    "    print(chunk_size)\n",
    "    \n",
    "    # 2D random array with positions (x, y)\n",
    "    xy = da.random.uniform(low=0.0, high=1.0, size=rand_array_shape, chunks=chunk_size)\n",
    "    print(f\" Created xy\\n {xy}\\n\")\n",
    "    print(f\" Number of partitions/chunks is {xy.numblocks}\\n\")\n",
    "    \n",
    "    \n",
    "    # check if position (x, y) is in unit circle\n",
    "    xy_inside_circle = (xy ** 2).sum(axis=1) < 1\n",
    "\n",
    "    # pi is the fraction of points in circle x 4\n",
    "    pi = 4 * xy_inside_circle.sum() / xy_inside_circle.size\n",
    "    \n",
    "    result = pi.compute()\n",
    "\n",
    "    print(f\"\\nfrom {xy.nbytes / 1e9} GB randomly chosen positions\")\n",
    "    print(f\"   pi estimate: {result}\")\n",
    "    print(f\"   pi error: {abs(result - np.pi)}\\n\")\n",
    "    \n",
    "    return result\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    cluster = LocalCUDACluster()             # Added\n",
    "    print(cluster)\n",
    "    \n",
    "    client = Client(cluster)\n",
    "    print(client)\n",
    "\n",
    "    t0 = time.time()\n",
    "    dask_calculate_pi(100000000000,40)\n",
    "    t1 = time.time()\n",
    "    print(\"time taken for dask is \" + str(t1-t0))\n",
    "\n",
    "    client.restart()\n",
    "    client.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary\n",
    "\n",
    "- We have seen that we can access the GPU(s) from python.\n",
    "\n",
    "- There is a tradeoff between ease of use and flexibility.\n",
    "\n",
    "- GPUs are more difficult to gnerate speedups over multithreading.\n",
    "\n",
    "- Fortunately there are packages/frameworks that have already been optimised for GPUs, *e.g.* tensorflow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Links\n",
    "\n",
    "__[GPU Architecture](https://core.vmware.com/resource/exploring-gpu-architecture#section1)__\n",
    "\n",
    "__[CUDA Basics](https://developer.nvidia.com/blog/cuda-refresher-cuda-programming-model/)__\n",
    "\n",
    "__[PyCUDA](https://documen.tician.de/pycuda/)__\n",
    "\n",
    "__[PyCUDA Device interface](https://documen.tician.de/pycuda/driver.html#pycuda.driver.Function)__\n",
    "\n",
    "__[Dask GPU](https://docs.dask.org/en/stable/gpu.html)__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
