{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"../../fig/ICHEC_Logo.jpg\" alt=\"Drawing\" style=\"width: 500px;\"/>\n",
    "\n",
    "# <center>Dask MPI<center/>\n",
    "******\n",
    "***\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- Up to now with Dask we have been using a single node.\n",
    "\n",
    "- The local cluster approach will not work over multiple nodes.\n",
    "\n",
    "- The issue is how the workers comminucate with the scheduler.\n",
    "\n",
    "- As we have seen we can use multiple nodes using MPI."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Operation\n",
    "\n",
    "- Dask mpi splits the scheduler and workers over MPI processes.\n",
    "\n",
    "- Rank 0 runs the scheduler.\n",
    "\n",
    "- Rank 1 runs the python script.\n",
    "\n",
    "- Ranks 2 and above are the workers.\n",
    "\n",
    "- Dask mpi is built on top of mpi4py."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate Pi\n",
    "\n",
    "- To illustrate how this is done we will use the previous example.\n",
    "\n",
    "- Below is the complete python code. \n",
    "\n",
    "- There is only one more package to import."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile dask_MPI_calculate_pi.py\n",
    "import numpy as np\n",
    "\n",
    "import dask\n",
    "import dask.array as da\n",
    "from dask.distributed import Client\n",
    "from dask_mpi import initialize\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The function itself is unchanged from last time when we used dask arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile -a dask_MPI_calculate_pi.py\n",
    "\n",
    "def dask_calculate_pi(size_in_bytes,nchunks):\n",
    "    \n",
    "    \"\"\"Calculate pi using a Monte Carlo method.\"\"\"\n",
    "    \n",
    "    rand_array_shape = (int(size_in_bytes / 8 / 2), 2)\n",
    "    chunk_size = int(rand_array_shape[0]/nchunks)\n",
    "    print(chunk_size)\n",
    "    \n",
    "    # 2D random array with positions (x, y)\n",
    "    xy = da.random.uniform(low=0.0, high=1.0, size=rand_array_shape, chunks=chunk_size)\n",
    "    print(f\" Created xy\\n {xy}\\n\")\n",
    "    print(f\" Number of partitions/chunks is {xy.numblocks}\\n\")\n",
    "    \n",
    "    \n",
    "    # check if position (x, y) is in unit circle\n",
    "    xy_inside_circle = (xy ** 2).sum(axis=1) < 1\n",
    "\n",
    "    # pi is the fraction of points in circle x 4\n",
    "    pi = 4 * xy_inside_circle.sum() / xy_inside_circle.size\n",
    "    \n",
    "    result = pi.compute()\n",
    "\n",
    "    print(f\"\\nfrom {xy.nbytes / 1e9} GB randomly chosen positions\")\n",
    "    print(f\"   pi estimate: {result}\")\n",
    "    print(f\"   pi error: {abs(result - np.pi)}\\n\")\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The changes come in the main body of the code.\n",
    "\n",
    "- You still need to initialise the scheduler and workers by calling Client.\n",
    "\n",
    "- However this time it is called without any arguments.\n",
    "\n",
    "- The system of MPI processes is created by calling initialize.\n",
    "\n",
    "- You can see that the parameters are setup at this call and not through Client().\n",
    "\n",
    "- One thing you may notice is that the number of workers has not been set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile -a dask_MPI_calculate_pi.py\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    initialize(nthreads=4,memory_limit='40GB')\n",
    "\n",
    "    client = Client()\n",
    "\n",
    "\n",
    "    t0 = time.time()\n",
    "    print(client)\n",
    "    dask_calculate_pi(100000000000,40)\n",
    "    t1 = time.time()\n",
    "    print(\"time taken for dask is \" + str(t1-t0))\n",
    "    \n",
    "    close.client()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- This is set at execute time.\n",
    "\n",
    "- Below is an example python script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile dask_MPI_calculate_pi.slurm\n",
    "#!/bin/bash\n",
    "#SBATCH --nodes=2\n",
    "#SBATCH --time=01:00:00\n",
    "#SBATCH -A course\n",
    "#SBATCH --job-name=calpi\n",
    "#SBATCH -p CourseDevQ\n",
    "#SBATCH --reservation=CourseMay\n",
    "\n",
    "\n",
    "module purge\n",
    "module load conda openmpi/gcc/3.1.2\n",
    "module list\n",
    "\n",
    "source activate /ichec/home/users/course00/conda_HPC\n",
    "\n",
    "cd $SLURM_SUBMIT_DIR\n",
    "\n",
    "\n",
    "mpirun -n 6 -npernode 3 python -u dask_MPI_calculate_pi.py\n",
    "\n",
    "exit 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The number of workers has been set by the number of processes we create.\n",
    "\n",
    "- In this case it is 4 because rank 0 is the scheduler and rank 1 runs the python script.\n",
    "\n",
    "- The workers come into play when there are parallel tasks to run.\n",
    "\n",
    "- Just to prove that it will work over multiple nodes I have asked for 3 processes to run per node.\n",
    "\n",
    "- This version is not faster than using plain dask but it allows more memory per worker."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "\n",
    "- We cannot run this example through the notebook.\n",
    "\n",
    "- But you can prove to yourself by running the cells above and submitting the sluem script.\n",
    "\n",
    "|<img src=\"../../fig/notebooks/Terminalicon2.png\" height=100 width=100>|\n",
    "|:--:|\n",
    "| dask_MPI_calculate_pi |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary\n",
    "\n",
    "- We can use dask over multiple nodes in Kay by using dask-mpi.\n",
    "\n",
    "- This is more scalable than the LocalCluster in terms of number of workers and memory per worker.\n",
    "\n",
    "- Using dask-array we can handle larger than memory problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Links\n",
    "\n",
    "__[Dask MPI](http://mpi.dask.org/en/latest/)__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
